{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 3: Non-Linear Models and Validation Metrics (37 total marks)\n",
    "### Due: October 24 at 11:59pm\n",
    "\n",
    "### Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses non-linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf275ca7",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2b67a661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee2d2c3",
   "metadata": {},
   "source": [
    "## Part 1: Regression (14.5 marks)\n",
    "\n",
    "For this section, we will be continuing with the concrete example from yellowbrick. You will need to compare these results to the results from the previous assignment. Please use the results from the solution if you were unable to complete Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219f163",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (0.5 marks)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the concrete dataset into the feature matrix `X` and target vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2af8bd32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TO DO: Import concrete dataset from yellowbrick library\n",
    "from yellowbrick.datasets import load_concrete\n",
    "\n",
    "X, y = load_concrete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea4cc",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0 marks)\n",
    "\n",
    "Data processing was completed in the previous assignment. No need to repeat here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a245d00",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import the Decision Tree, Random Forest and Gradient Boosting Machines regression models from sklearn\n",
    "2. Instantiate the three models with `max_depth = 5`. Are there any other parameters that you will need to set?\n",
    "3. Implement each machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f994e31",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the average training and validation accuracy using mean squared error with cross-validation. To do this, you will need to set `scoring='neg_mean_squared_error'` in your `cross_validate` function and negate the results (multiply by -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc3f7a8",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: DT, RF and GB\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "fdc93a78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Training accuracy  Validation accuracy\n",
      "DT          49.862596            79.490160\n",
      "RF          31.540857            50.082127\n",
      "GB           3.657019            22.141648\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Instantiate Decision Tree model with max_depth\n",
    "decision_tree_model = DecisionTreeRegressor(max_depth=5, random_state=0)\n",
    "\n",
    "# Instantiate Random Forest model with max_depth\n",
    "random_forest_model = RandomForestRegressor(max_depth=5, random_state=0)\n",
    "\n",
    "# Instantiate Gradient Boosting model with max_depth\n",
    "gradient_boosting_model = GradientBoostingRegressor(max_depth=5, random_state=0)\n",
    "\n",
    "# Fit the models to the data\n",
    "decision_tree_model.fit(X,y)\n",
    "random_forest_model.fit(X,y)\n",
    "gradient_boosting_model.fit(X,y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "results_list = []\n",
    "    \n",
    "for model in [decision_tree_model, random_forest_model,  gradient_boosting_model]:\n",
    "\n",
    "    scores = cross_validate(model, X_train, y_train, cv=5, \n",
    "                        scoring='neg_mean_squared_error',\n",
    "                       return_train_score=True)\n",
    "    \n",
    "    \n",
    "    train_sc = -scores['train_score'].mean()\n",
    "    val_sc = -scores['test_score'].mean()\n",
    "\n",
    "    results_list.append([train_sc, val_sc])\n",
    "                    \n",
    "results = pd.DataFrame(results_list, columns=['Training accuracy', 'Validation accuracy'], index = ['DT', 'RF', 'GB'])\n",
    "print(results)    \n",
    "\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31715a9d",
   "metadata": {},
   "source": [
    "Repeat the step above to print the R2 score instead of the mean-squared error. For this case, you can use `scoring='r2'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "83539f47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Training accuracy  Validation accuracy\n",
      "DT           0.821295             0.710786\n",
      "RF           0.886982             0.819498\n",
      "GB           0.986913             0.919990\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "results_list = []\n",
    "    \n",
    "for model in [decision_tree_model, random_forest_model,  gradient_boosting_model]:\n",
    "\n",
    "    scores = cross_validate(model, X_train, y_train, cv=5, \n",
    "                        scoring='r2',\n",
    "                       return_train_score=True)\n",
    "    \n",
    "    \n",
    "    train_sc = scores['train_score'].mean()\n",
    "    val_sc = scores['test_score'].mean()\n",
    "\n",
    "    results_list.append([train_sc, val_sc])\n",
    "                    \n",
    "results = pd.DataFrame(results_list, columns=['Training accuracy', 'Validation accuracy'], index = ['DT', 'RF', 'GB'])\n",
    "print(results)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5257a98",
   "metadata": {},
   "source": [
    "### Questions (6 marks)\n",
    "1. How do these results compare to the results using a linear model in the previous assignment? Use values.\n",
    "1. Out of the models you tested, which model would you select for this dataset and why?\n",
    "1. If you wanted to increase the accuracy of the tree-based models, what would you do? Provide two suggestions.\n",
    "\n",
    "*ANSWER HERE*\n",
    "1. These results seem better than the results using a linear model in the previous assignment. The mse values for both training and validation data sets were 110 and 96 for the linear model and the mse values for both training and validation accuracy for the three models were all significantly lower at 50 and 79 for the DT model, 32 and 50 for the RF model and 4 and 22 for the GB model. The r2 values for both training and validation data sets were 0.61 and 0.64 for the linear model and the r2 values for both training and validation accuracy for the three models were all significantly higher at 0.82 and 0.71 for the DT model, 0.89 and 0.82 for the RF model and 0.99 and 0.92 for the GB model.\n",
    "\n",
    "2. I would select the GB model as it had the highest r2 and lowest mse values for both training and validation accuracy.\n",
    "\n",
    "3. To increase the accuracy of tree-based models, which have a tendency to overfit, I would pre prune further by reducing the max_depth to 4 and I would do some post pruning and remove nodes that contain little to no information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b238f4",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93097bfe",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\n",
    "\n",
    "I sourced my code from my previous assignment and the Decision Trees Example.ipynb. I completed the assignment in the steps listed. I didn't really have many challenges as the previous assignment and the Decision Trees Example covered the code and the lecture notes had all the information needed to answer the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 2: Classification (17.5 marks)\n",
    "\n",
    "You have been asked to develop code that can help the user classify different wine samples. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (2 marks)\n",
    "\n",
    "The data used for this task can be downloaded from UCI: https://archive.ics.uci.edu/dataset/109/wine\n",
    "\n",
    "Use the pandas library to load the dataset. You must define the column headers if they are not included in the dataset \n",
    "\n",
    "You will need to split the dataset into feature matrix `X` and target vector `y`. Which column represents the target vector?\n",
    "\n",
    "Print the size and type of `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "33583c67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X size=(178, 13); type=<class 'pandas.core.frame.DataFrame'>\n",
      "y size=(178,); type=<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import wine dataset\n",
    "\n",
    "df = pd.read_csv('wine.data', \n",
    "                 names = ['class', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', \n",
    "                          'Total phenols', 'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n",
    "                          'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline'])\n",
    "df\n",
    "\n",
    "features = ['Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', \n",
    "            'Total phenols', 'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n",
    "            'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline']\n",
    "X = df.loc[:, features]\n",
    "\n",
    "y = df['class']\n",
    "\n",
    "print(f\"X size={X.shape}; type={type(X)}\")\n",
    "print(f\"y size={y.shape}; type={type(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28af110",
   "metadata": {},
   "source": [
    "Print the first five rows of the dataset to inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ea266921",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   class  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
      "0      1    14.23        1.71  2.43               15.6        127   \n",
      "1      1    13.20        1.78  2.14               11.2        100   \n",
      "2      1    13.16        2.36  2.67               18.6        101   \n",
      "3      1    14.37        1.95  2.50               16.8        113   \n",
      "4      1    13.24        2.59  2.87               21.0        118   \n",
      "\n",
      "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
      "0           2.80        3.06                  0.28             2.29   \n",
      "1           2.65        2.76                  0.26             1.28   \n",
      "2           2.80        3.24                  0.30             2.81   \n",
      "3           3.85        3.49                  0.24             2.18   \n",
      "4           2.80        2.69                  0.39             1.82   \n",
      "\n",
      "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
      "0             5.64  1.04                          3.92     1065  \n",
      "1             4.38  1.05                          3.40     1050  \n",
      "2             5.68  1.03                          3.17     1185  \n",
      "3             7.80  0.86                          3.45     1480  \n",
      "4             4.32  1.04                          2.93      735  \n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fc8fe",
   "metadata": {},
   "source": [
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "97c6e9dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alcohol                         0\n",
      "Malic acid                      0\n",
      "Ash                             0\n",
      "Alcalinity of ash               0\n",
      "Magnesium                       0\n",
      "Total phenols                   0\n",
      "Flavanoids                      0\n",
      "Nonflavanoid phenols            0\n",
      "Proanthocyanins                 0\n",
      "Color intensity                 0\n",
      "Hue                             0\n",
      "OD280/OD315 of diluted wines    0\n",
      "Proline                         0\n",
      "dtype: int64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "print(X.isnull().sum())\n",
    "print(y.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070956af",
   "metadata": {},
   "source": [
    "How many samples do we have of each type of wine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b37a6fd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "1    59\n",
      "2    71\n",
      "3    48\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE, Assuming type is class\n",
    "print(df.groupby('class').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `SVC` and `DecisionTreeClassifier` from sklearn\n",
    "2. Instantiate models as `SVC()` and `DecisionTreeClassifier(max_depth = 3)`\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0870b0d2",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model \n",
    "\n",
    "Calculate the average training and validation accuracy using `cross_validate` for the two different models listed in Step 3. For this case, use `scoring='accuracy'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0bbd83",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "#### Step 5.1: Compare Models\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "be4b5c0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Data size  Training accuracy  Validation accuracy\n",
      "DTC  (178, 13)           0.993750              0.91875\n",
      "SVC  (178, 13)           0.682813              0.66250\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "\n",
    "dtc_model = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
    "dtc_model.fit(X,y)\n",
    "\n",
    "svc_model = SVC()\n",
    "svc_model.fit(X,y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "results_list = []\n",
    "    \n",
    "for model in [dtc_model, svc_model]:\n",
    "\n",
    "    scores = cross_validate(model, X_train, y_train, cv=5, \n",
    "                        scoring='accuracy',\n",
    "                       return_train_score=True)\n",
    "    \n",
    "    \n",
    "    train_sc = scores['train_score'].mean()\n",
    "    val_sc = scores['test_score'].mean()\n",
    "\n",
    "    results_list.append([X.shape,train_sc, val_sc])\n",
    "                    \n",
    "results = pd.DataFrame(results_list, columns=['Data size', 'Training accuracy', 'Validation accuracy'], index = ['DTC', 'SVC'])\n",
    "print(results)  \n",
    "\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e17878",
   "metadata": {},
   "source": [
    "#### Step 5.2: Visualize Classification Errors\n",
    "Which method gave the highest accuracy? Use this method to print the confusion matrix and classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "44b091a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {color: black;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=3, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=3, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=3, random_state=0)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: Implement best model\n",
    "dtc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "09d21b59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(170.97222222222223, 0.5, 'true value')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TO DO: Print confusion matrix using a heatmap\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "mat = confusion_matrix(y_test, dtc_model.predict(X_test))\n",
    "\n",
    "sns.heatmap(mat, square=True, annot=True, cbar=False)\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5ef95947",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00         6\n",
      "           2       1.00      1.00      1.00         7\n",
      "           3       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        18\n",
      "   macro avg       1.00      1.00      1.00        18\n",
      "weighted avg       1.00      1.00      1.00        18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Print classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, dtc_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf319621",
   "metadata": {},
   "source": [
    "### Questions (6 marks)\n",
    "1. How do the training and validation accuracy change depending on the method used? Explain with values.\n",
    "1. What are two reasons why the support vector machines model did not work as well as the tree-based model?\n",
    "1. How many samples were incorrectly classified in step 5.2? \n",
    "1. In this case, is maximizing precision or recall more important? Why?\n",
    "\n",
    "*YOUR ANSWERS HERE*\n",
    "1. The training and validation accuracy was higher using the decision tree classifier than the svc model. The decision tree classifier yielded training and validation accuracies of 0.99 and 0.92, whereas the svc model yielded yielded training and validation accuracies of 0.68 and 0.66.\n",
    "\n",
    "2. The support vector machine model could have not worked as well because the data might not have been easily linearly seperable and also becuase SVMs are very sensitive to feature ranges and we didn't scale them.\n",
    "\n",
    "3. None\n",
    "\n",
    "4. Both are equally as important as the consequences of a false positive and false negative are the same (the guessed location of the wine is wrong)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ff8ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e837da",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\n",
    "I sourced my code using part 1, Regression Metrics-filled.ipynb and Decision Trees Example.ipynb. I completed the steps in the order they were listed. I used generative AI to clear up some concepts such as precision and recall, I mostly used the lecture slides including classification uncertainity and did not modify my code. The biggest challenges I had were: understanding data size for part 2 step 5.1 and understanding a confusion matrix that is 3x3. I'm not sure if I overcame the above challenges or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd7358d",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "*ADD YOUR FINDINGS HERE*\n",
    "1. Pre pruning decision tree models helps to decrease overfitting.\n",
    "2. Support vector machine models require careful preprocessing of data and tuning of parameters to be accurate as mentioned in the SVM slides.\n",
    "3. Ensemble of decision trees tend to work better than single decision trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97b6ac",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*\n",
    "I liked how the assignment was broken down into smaller steps. I found the confusion matrix portion quite confusing as there were three types of classification so I didn't understand what a false positive and false negative were really well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa21e53b",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (3 marks)\n",
    "\n",
    "Repeat Part 2 and compare the support vector machines model used to `LinearSVC(max_iter=5000)`. Does using `LinearSVC` improve the results? Why or why not?\n",
    "\n",
    "Is `LinearSVC` a good fit for this dataset? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30fea72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc68a4",
   "metadata": {},
   "source": [
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c3b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
